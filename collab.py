# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YFOX_H1FciD4EZxVNm4WdPfKd878BeqN
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
!pip install flower datasets tensorboard

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/facebookresearch/dino.git
# %cd dino

import torch
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.utils.data import random_split, Subset
import numpy as np

# Transforms for CIFAR-100
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
])

# Download datasets
full_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)

# Split train into train (45k) and val (5k)
trainset, valset = random_split(full_trainset, [45000, 5000])

# Function for IID sharding (K clients)
def iid_sharding(dataset, K):
    num_samples = len(dataset)
    samples_per_client = num_samples // K
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    client_data = [Subset(dataset, indices[i*samples_per_client:(i+1)*samples_per_client]) for i in range(K)]
    return client_data

# Function for non-IID sharding (Nc classes per client)
def non_iid_sharding(dataset, K, Nc):
    labels = np.array(dataset.targets)
    class_indices = [np.where(labels == c)[0] for c in range(100)]
    np.random.shuffle(class_indices)
    client_classes = np.array_split(np.arange(100), K)
    client_data = []
    for client in range(K):
        client_indices = []
        for c in client_classes[client][:Nc]:
            client_indices.extend(class_indices[c][:len(class_indices[c]) // K])
        client_data.append(Subset(dataset, client_indices))
    return client_data

# Example usage (adjust as needed)
# clients_iid = iid_sharding(trainset, K=100)
# clients_non_iid = non_iid_sharding(trainset, K=100, Nc=5)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler  # Updated import for autocast
from torch.utils.tensorboard import SummaryWriter  # For logging
from vision_transformer import vit_small  # From dino repo (adjust import if needed)
import time
import os
import math  # For NaN checks in early stopping

# Paths
drive_path = '/content/drive/MyDrive/fl_project/'
checkpoint_path = os.path.join(drive_path, 'centralized_checkpoint.pth')
log_dir = os.path.join(drive_path, 'logs')  # For TensorBoard
os.makedirs(log_dir, exist_ok=True)
writer = SummaryWriter(log_dir)

# Model, optimizer, scheduler
model = vit_small(patch_size=16, num_classes=100)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)  # Adjusted for max epochs
scaler = GradScaler()  # For mixed precision

# Data loaders (assume trainset, valset, testset from earlier preparation)
trainloader = DataLoader(trainset, batch_size=128, shuffle=True)
valloader = DataLoader(valset, batch_size=128)
testloader = DataLoader(testset, batch_size=128)

# Load checkpoint if exists
start_epoch = 0
best_val_loss = math.inf
patience_counter = 0
patience = 20  # For early stopping
if os.path.exists(checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
    best_val_loss = checkpoint.get('best_val_loss', math.inf)
    patience_counter = checkpoint.get('patience_counter', 0)
    print(f"Resumed from epoch {start_epoch} with best val loss {best_val_loss:.4f}")

num_epochs = 300  # Max; early stopping will halt if needed
total_start_time = time.time()

for epoch in range(start_epoch, num_epochs):
    model.train()
    epoch_start_time = time.time()
    running_loss = 0.0
    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        with autocast():  # Fixed syntax
            outputs = model(inputs)
            loss = criterion(outputs, labels)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        running_loss += loss.item()

    train_loss = running_loss / len(trainloader)
    scheduler.step()

    # Validation
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for inputs, labels in valloader:
            inputs, labels = inputs.to(device), labels.to(device)
            with autocast():  # Fixed syntax
                outputs = model(inputs)
                loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    val_loss /= len(valloader)
    val_acc = 100. * correct / total

    # Log to TensorBoard
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/val', val_loss, epoch)
    writer.add_scalar('Accuracy/val', val_acc, epoch)

    epoch_time = time.time() - epoch_start_time
    print(f"Epoch {epoch+1}/{num_epochs} completed. Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%. Time: {epoch_time/60:.2f} minutes")

    # Early stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        torch.save(model.state_dict(), os.path.join(drive_path, 'best_model.pth'))  # Save best model
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"Early stopping triggered at epoch {epoch+1}")
            break

    # Checkpoint every 10 epochs or at end
    if (epoch + 1) % 10 == 0 or (epoch + 1) == num_epochs or patience_counter >= patience:
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'best_val_loss': best_val_loss,
            'patience_counter': patience_counter
        }, checkpoint_path)
        print(f"Checkpoint saved at epoch {epoch+1}")

# Final evaluation on test set
model.eval()
test_loss = 0.0
correct = 0
total = 0
with torch.no_grad():
    for inputs, labels in testloader:
        inputs, labels = inputs.to(device), labels.to(device)
        with autocast():  # Fixed syntax
            outputs = model(inputs)
            loss = criterion(outputs, labels)
        test_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

test_loss /= len(testloader)
test_acc = 100. * correct / total
print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%")

# Final save and total time
torch.save(model.state_dict(), os.path.join(drive_path, 'centralized_model_final.pth'))
total_time = time.time() - total_start_time
print(f"Training completed. Total epochs: {epoch - start_epoch + 1}. Total time: {total_time/3600:.2f} hours")

writer.close()  # Close TensorBoard writer

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir /content/drive/MyDrive/fl_project/logs

from torch.optim import SGD
import torch

class SparseSGDM(SGD):
    def __init__(self, params, lr=0.01, momentum=0.9, sparsity_ratio=0.5):
        super(SparseSGDM, self).__init__(params, lr=lr, momentum=momentum)
        self.gradient_masks = {}
        # Generate masks for each parameter
        for group in self.param_groups:
            for p in group['params']:
                if p.requires_grad:
                    mask = (torch.rand_like(p.data) > sparsity_ratio).float()  # 1 for update, 0 for zero
                    self.gradient_masks[id(p)] = mask

    def step(self, closure=None):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    mask = self.gradient_masks.get(id(p))
                    if mask is not None:
                        p.grad.data.mul_(mask)
        super(SparseSGDM, self).step(closure)

# Load centralized model
model.load_state_dict(torch.load('/content/drive/MyDrive/fl_project/centralized_model_final.pth', weights_only=True))  # Fix FutureWarning
model.to(device)

# Create optimizer with sparsity
optimizer = SparseSGDM(model.parameters(), lr=0.001, momentum=0.9, sparsity_ratio=0.5)

# Quick test loop (1 epoch on small subset)
model.train()
for inputs, labels in DataLoader(trainset, batch_size=32, shuffle=True):  # Small batch for test
    inputs, labels = inputs.to(device), labels.to(device)
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = nn.CrossEntropyLoss()(outputs, labels)
    loss.backward()
    optimizer.step()
    break  # Just one batch for testing
print("SparseSGDM test completed successfully.")

import torch

def calibrate_mask(model, dataloader, threshold=0.1, rounds=3):
    model.eval()
    masks = {}
    for _ in range(rounds):  # Multiple rounds for stability
        sensitivities = {}
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = nn.CrossEntropyLoss()(outputs, labels)
            loss.backward()
            for name, param in model.named_parameters():
                if param.grad is not None:
                    sensitivities[name] = sensitivities.get(name, 0) + (param.grad.data.norm(2) ** 2)
            model.zero_grad()
        # Create mask: 1 for low-sensitivity (below threshold), 0 otherwise
        for name in sensitivities:
            avg_sens = sensitivities[name] / len(dataloader)
            mask = (avg_sens < threshold).float()
            masks[name] = mask
    return masks  # Dictionary of masks per parameter

# Example usage
trainloader_small = DataLoader(trainset, batch_size=32, shuffle=True)  # Small loader for calibration
masks = calibrate_mask(model, trainloader_small, threshold=0.2, rounds=2)
print("Gradient masks calibrated.")

from torch.optim import SGD
from torch.cuda.amp import autocast  # Correct import for new syntax
import torch.nn as nn

# Updated SparseSGDM to handle mask dict (unchanged from your code)
class SparseSGDM(SGD):
    def __init__(self, params, lr=0.01, momentum=0.9, gradient_masks=None):
        super(SparseSGDM, self).__init__(params, lr=lr, momentum=momentum)
        self.gradient_masks = gradient_masks or {}

    def step(self, closure=None):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    name = [name for name, param in model.named_parameters() if param is p][0]
                    mask = self.gradient_masks.get(name)
                    if mask is not None:
                        p.grad.data.mul_(mask)
        super(SparseSGDM, self).step(closure)

optimizer = SparseSGDM(model.parameters(), lr=0.001, momentum=0.9, gradient_masks=masks)

# Fine-tune (10 epochs example)
num_fine_epochs = 10
for epoch in range(num_fine_epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        with autocast():  # Fixed syntax
            outputs = model(inputs)
            loss = criterion(outputs, labels)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
    print(f"Fine-tune Epoch {epoch+1}: Loss {running_loss / len(trainloader):.4f}")

torch.save(model.state_dict(), '/content/drive/MyDrive/fl_project/sparse_finetuned_model.pth')

from torch.optim import SGD
import torch

class SparseSGDM(SGD):
    def __init__(self, params, lr=0.01, momentum=0.9, gradient_masks=None):
        super(SparseSGDM, self).__init__(params, lr=lr, momentum=momentum)
        self.gradient_masks = gradient_masks or {}

    def step(self, closure=None):
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is not None:
                    name = [name for name, param in model.named_parameters() if param is p][0]
                    mask = self.gradient_masks.get(name)
                    if mask is not None:
                        p.grad.data.mul_(mask)
        super(SparseSGDM, self).step(closure)

!pip install flwr

import flwr as fl
from torch.utils.data import DataLoader
import torch.nn as nn  # Correct import for CrossEntropyLoss

class FlowerClient(fl.client.NumPyClient):
    def __init__(self, cid, trainloader, valloader, masks):
        self.model = vit_small(patch_size=16, num_classes=100).to(device)
        self.optimizer = SparseSGDM(self.model.parameters(), lr=0.01, momentum=0.9, gradient_masks=masks)
        self.trainloader = trainloader
        self.valloader = valloader

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        self.model.train()
        for _ in range(4):  # J=4 local steps
            for inputs, labels in self.trainloader:
                inputs, labels = inputs.to(device), labels.to(device)
                self.optimizer.zero_grad()
                with autocast():
                    outputs = self.model(inputs)
                    loss = nn.CrossEntropyLoss()(outputs, labels)
                scaler.scale(loss).backward()
                scaler.step(self.optimizer)
                scaler.update()
        return self.get_parameters(config={}), len(self.trainloader.dataset), {}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        self.model.eval()
        loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for inputs, labels in self.valloader:
                inputs, labels = inputs.to(device), labels.to(device)
                with autocast():
                    outputs = self.model(inputs)
                    loss += nn.CrossEntropyLoss()(outputs, labels).item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        return float(loss / len(self.valloader)), len(self.valloader.dataset), {"accuracy": float(correct / total)}

print("Imports fixed and FlowerClient class defined successfully.")

from torch.utils.data import DataLoader
import flwr as fl
import torch.nn as nn
from torch.cuda.amp import autocast
from torch.utils.data import random_split, Subset
import numpy as np

# Transforms for CIFAR-100 (assuming they are needed)
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
])

# Download datasets (assuming they are needed and not already downloaded)
import torchvision.datasets as datasets
full_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)

# Split train into train (45k) and val (5k)
trainset, valset = random_split(full_trainset, [45000, 5000])

# Function for IID sharding (K clients)
def iid_sharding(dataset, K):
    num_samples = len(dataset)
    samples_per_client = num_samples // K
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    client_data = [Subset(dataset, indices[i*samples_per_client:(i+1)*samples_per_client]) for i in range(K)]
    return client_data

# Assuming vit_small and device are defined in a previous cell and accessible

# FlowerClient class
class FlowerClient(fl.client.NumPyClient):
    def __init__(self, cid, trainloader, valloader, masks):
        self.model = vit_small(patch_size=16, num_classes=100).to(device)
        # Assuming SparseSGDM is defined in a previous cell and accessible
        self.optimizer = SparseSGDM(self.model.parameters(), lr=0.01, momentum=0.9, gradient_masks=masks)
        self.trainloader = trainloader
        self.valloader = valloader

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        self.model.train()
        # Assuming scaler is defined in a previous cell and accessible
        for _ in range(4):  # J=4 local steps
            for inputs, labels in self.trainloader:
                inputs, labels = inputs.to(device), labels.to(device)
                self.optimizer.zero_grad()
                with autocast():
                    outputs = self.model(inputs)
                    loss = nn.CrossEntropyLoss()(outputs, labels)
                scaler.scale(loss).backward()
                scaler.step(self.optimizer)
                scaler.update()
        return self.get_parameters(config={}), len(self.trainloader.dataset), {}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        self.model.eval()
        loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for inputs, labels in self.valloader:
                inputs, labels = inputs.to(device), labels.to(device)
                with autocast():
                    outputs = self.model(inputs)
                    loss += nn.CrossEntropyLoss()(outputs, labels).item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        return float(loss / len(self.valloader)), len(self.valloader.dataset), {"accuracy": float(correct / total)}

# Create IID clients and loaders
clients_iid = iid_sharding(trainset, K=100)
client_loaders = [(DataLoader(clients_iid[i], batch_size=128, shuffle=True, num_workers=2, pin_memory=True),
                   DataLoader(valset, batch_size=128, num_workers=2, pin_memory=True)) for i in range(100)]

# Client function
def client_fn(cid: str):
    # Assuming masks is defined in a previous cell and accessible
    return FlowerClient(int(cid), *client_loaders[int(cid)], masks)  # Pass your calibrated masks

# Start simulation (50 rounds; reduce to 10-20 if testing)
fl.simulation.start_simulation(
    client_fn=client_fn,
    num_clients=100,
    client_resources={"num_cpus": 1, "num_gpus": 1.0},
    config=fl.server.ServerConfig(num_rounds=50)
)

# Save final global model (add after simulation if needed)
# Assuming model is accessible here (it's defined within the client, so need to adapt)
# For saving the final model, you would typically get the aggregated parameters from the server
# and load them into a model instance after the simulation.
# Example (would need to retrieve parameters from simulation result):
# final_parameters = simulation_result.parameters # This is conceptual; need to check Flower docs
# final_model = vit_small(patch_size=16, num_classes=100)
# params_dict = zip(final_model.state_dict().keys(), final_parameters)
# state_dict = {k: torch.tensor(v) for k, v in params_dict}
# final_model.load_state_dict(state_dict, strict=True)
# torch.save(final_model.state_dict(), '/content/drive/MyDrive/fl_project/fedavg_iid_model.pth')

print("Simulation completed.")

!pip install -U "flwr[simulation]"

"""# Task
Analyze the provided project requirements document "/content/Project 5_ Federated Learning Under the Lens of Model Editing.docx" and the current state of the notebook to determine the completed and remaining steps of the project.

## Analyze project requirements

### Subtask:
Read the document "/content/Project 5_ Federated Learning Under the Lens of Model Editing.docx" to identify the key requirements and stages of the project.

## Review notebook state

### Subtask:
Examine the executed cells in the provided notebook to understand the completed steps.

## Compare requirements and notebook state

### Subtask:
Compare the identified project requirements with the executed notebook cells to determine which requirements have been met.

**Reasoning**:
Compare the executed notebook cells with the identified high-level project requirements to determine which have been met. Since I cannot access the document content, the comparison is limited to the observable progress in the notebook.
"""

# Based on the analysis of the executed notebook cells:

print("Comparison of project requirements (based on notebook state) and executed steps:")
print("- Environment setup (libraries, CUDA): Met (Cell 8WaQWgiQ7WlW, htpHSh_jAGC8, vVZSyKqBTnF4, 313fc8f3)")
print("- Data loading and processing (CIFAR-100, train/val/test split, sharding functions): Met (Cell R7aux2uhBCai, JK4zR2nvVPwj)")
print("- Model readiness (definition, loading centralized model): Met (Cell QUs5FJwSBfkm, 0d2KbrzlK5Vi, 9y8C7QPCN5Hp, glO1qlnaUDjo)")
print("- Sparse optimizer definition and mask calibration: Met (Cell A1HY1tvLK3kF, RCoBla5CL0au, McKXLe2FThD9, 9y8C7QPCN5Hp)")
print("- Federated Learning Client class definition: Met (Cell glO1qlnaUDjo, JK4zR2nvVPwj)")
print("- Initial Federated Learning simulation attempt: Attempted but failed due to missing dependency (Cell JK4zR2nvVPwj)")

print("\nNote: A detailed comparison of all project requirements from the .docx document is not possible due to the inability to access the document content in the previous step. The comparison is limited to the observable progress in the notebook.")

"""## Identify remaining steps

### Subtask:
Based on the comparison, outline the remaining steps required to complete the project according to the document.

**Reasoning**:
Based on the executed notebook and the previous analysis, the main remaining step is to successfully run the federated learning simulation using Flower, address the dependency issue that prevented the previous attempt, configure the simulation, run it, and then evaluate the final aggregated model. This involves installing the necessary dependency, configuring the server strategy, initiating the simulation, evaluating the resulting model, and potentially exploring model editing techniques.
"""

import flwr as fl
import torch
import torch.nn as nn
from torch.cuda.amp import autocast
from torch.utils.data import DataLoader
from torch.utils.data import random_split, Subset
import numpy as np

# Assuming vit_small and device are defined in a previous cell and accessible
# Assuming SparseSGDM is defined in a previous cell and accessible
# Assuming trainset, valset, and testset are defined in a previous cell and accessible
# Assuming masks is defined in a previous cell and accessible
# Assuming scaler is defined in a previous cell and accessible

# Re-define FlowerClient class (to ensure all necessary components are present)
class FlowerClient(fl.client.NumPyClient):
    def __init__(self, cid, trainloader, valloader, masks):
        self.cid = cid
        self.model = vit_small(patch_size=16, num_classes=100).to(device)
        self.optimizer = SparseSGDM(self.model.parameters(), lr=0.01, momentum=0.9, gradient_masks=masks)
        self.trainloader = trainloader
        self.valloader = valloader

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        self.model.train()
        for epoch in range(config["local_epochs"]):  # Use local_epochs from config
            for inputs, labels in self.trainloader:
                inputs, labels = inputs.to(device), labels.to(device)
                self.optimizer.zero_grad()
                with autocast():
                    outputs = self.model(inputs)
                    loss = nn.CrossEntropyLoss()(outputs, labels)
                scaler.scale(loss).backward()
                scaler.step(self.optimizer)
                scaler.update()
        return self.get_parameters(config={}), len(self.trainloader.dataset), {}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        self.model.eval()
        loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for inputs, labels in self.valloader:
                inputs, labels = inputs.to(device), labels.to(device)
                with autocast():
                    outputs = self.model(inputs)
                    loss += nn.CrossEntropyLoss()(outputs, labels).item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        return float(loss / len(self.valloader)), len(self.valloader.dataset), {"accuracy": float(correct / total)}

# Create IID clients and loaders (re-run in case previous cell had issues)
K = 100
clients_iid = iid_sharding(trainset, K=K)
client_loaders = [(DataLoader(clients_iid[i], batch_size=64, shuffle=True, num_workers=2, pin_memory=True),
                   DataLoader(valset, batch_size=64, num_workers=2, pin_memory=True)) for i in range(K)]

# Client function
def client_fn(cid: str):
    return FlowerClient(int(cid), *client_loaders[int(cid)], masks)  # Pass your calibrated masks

# Define server strategy (FedAvg with evaluation)
strategy = fl.server.strategy.FedAvg(
    fraction_fit=0.1,  # Sample 10% of clients for training in each round
    fraction_evaluate=0.1, # Sample 10% of clients for evaluation in each round
    min_fit_clients=10, # Wait for minimum 10 clients to be available for fitting
    min_evaluate_clients=10, # Wait for minimum 10 clients to be available for evaluation
    min_available_clients=K, # Wait for all K clients to be available
    evaluate_fn=lambda parameters, config: None, # Disable server-side evaluation for now
    on_fit_config_fn=lambda server_round: {"local_epochs": 4} # Configure local training
)

# Start simulation (Reduce rounds for faster testing)
print("Starting Flower simulation...")
history = fl.simulation.start_simulation(
    client_fn=client_fn,
    num_clients=K,
    client_resources={"num_cpus": 1, "num_gpus": 1.0},
    config=fl.server.ServerConfig(num_rounds=10), # Reduced to 10 rounds
    strategy=strategy
)

print("Simulation completed.")

# Further steps will involve evaluating the history and potentially loading the final model for testing and model editing.

"""**Reasoning**:
The simulation failed again with the same `ImportError` related to `ray`, despite attempting to install `flwr[simulation]` previously. This indicates a potential issue with the installation or environment. Before proceeding with the simulation, it's necessary to ensure the `ray` library is correctly installed and accessible. I will try installing it directly.


"""

!pip install ray

"""**Reasoning**:
The previous command showed that `ray` is already installed and satisfied the requirements. The `ImportError` might be due to the Ray runtime not being initialized or accessible within the Colab environment in the way Flower expects. Re-running the simulation code might trigger the necessary Ray initialization or resolve a transient issue.


"""

import flwr as fl
import torch
import torch.nn as nn
from torch.cuda.amp import autocast
from torch.utils.data import DataLoader
from torch.utils.data import random_split, Subset
import numpy as np

# Assuming vit_small and device are defined in a previous cell and accessible
# Assuming SparseSGDM is defined in a previous cell and accessible
# Assuming trainset, valset, and testset are defined in a previous cell and accessible
# Assuming masks is defined in a previous cell and accessible
# Assuming scaler is defined in a previous cell and accessible
# Assuming K and client_loaders are defined in a previous cell and accessible

# Re-define FlowerClient class (to ensure all necessary components are present)
class FlowerClient(fl.client.NumPyClient):
    def __init__(self, cid, trainloader, valloader, masks):
        self.cid = cid
        self.model = vit_small(patch_size=16, num_classes=100).to(device)
        self.optimizer = SparseSGDM(self.model.parameters(), lr=0.01, momentum=0.9, gradient_masks=masks)
        self.trainloader = trainloader
        self.valloader = valloader

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        self.model.train()
        for epoch in range(config["local_epochs"]):  # Use local_epochs from config
            for inputs, labels in self.trainloader:
                inputs, labels = inputs.to(device), labels.to(device)
                self.optimizer.zero_grad()
                with autocast():
                    outputs = self.model(inputs)
                    loss = nn.CrossEntropyLoss()(outputs, labels)
                scaler.scale(loss).backward()
                scaler.step(self.optimizer)
                scaler.update()
        return self.get_parameters(config={}), len(self.trainloader.dataset), {}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        self.model.eval()
        loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for inputs, labels in self.valloader:
                inputs, labels = inputs.to(device), labels.to(device)
                with autocast():
                    outputs = self.model(inputs)
                    loss += nn.CrossEntropyLoss()(outputs, labels).item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        return float(loss / len(self.valloader)), len(self.valloader.dataset), {"accuracy": float(correct / total)}


# Client function
def client_fn(cid: str):
    return FlowerClient(int(cid), *client_loaders[int(cid)], masks)  # Pass your calibrated masks

# Define server strategy (FedAvg with evaluation)
strategy = fl.server.strategy.FedAvg(
    fraction_fit=0.1,  # Sample 10% of clients for training in each round
    fraction_evaluate=0.1, # Sample 10% of clients for evaluation in each round
    min_fit_clients=10, # Wait for minimum 10 clients to be available for fitting
    min_evaluate_clients=10, # Wait for minimum 10 clients to be available for evaluation
    min_available_clients=K, # Wait for all K clients to be available
    evaluate_fn=lambda parameters, config: None, # Disable server-side evaluation for now
    on_fit_config_fn=lambda server_round: {"local_epochs": 4} # Configure local training
)

# Start simulation (Reduce rounds for faster testing)
print("Starting Flower simulation...")
history = fl.simulation.start_simulation(
    client_fn=client_fn,
    num_clients=K,
    client_resources={"num_cpus": 1, "num_gpus": 1.0},
    config=fl.server.ServerConfig(num_rounds=10), # Reduced to 10 rounds
    strategy=strategy
)

print("Simulation completed.")

# Further steps will involve evaluating the history and potentially loading the final model for testing and model editing.

"""## Identify remaining steps

### Subtask:
Address the Ray import error and successfully run the federated learning simulation.

**Reasoning**:
Re-running the installation command for flwr with the simulation extra to ensure Ray is correctly installed for Flower.
"""

!pip install -U "flwr[simulation]"

"""**Reasoning**:
Re-run the cell containing the Flower simulation code to attempt to start the simulation again now that the flwr[simulation] package is confirmed to be installed.


"""

from torch.utils.data import DataLoader
import flwr as fl
import torch.nn as nn
from torch.cuda.amp import autocast
from torch.utils.data import random_split, Subset
import numpy as np

# Transforms for CIFAR-100 (assuming they are needed)
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
])

# Download datasets (assuming they are needed and not already downloaded)
import torchvision.datasets as datasets
full_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)

# Split train into train (45k) and val (5k)
trainset, valset = random_split(full_trainset, [45000, 5000])

# Function for IID sharding (K clients)
def iid_sharding(dataset, K):
    num_samples = len(dataset)
    samples_per_client = num_samples // K
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    client_data = [Subset(dataset, indices[i*samples_per_client:(i+1)*samples_per_client]) for i in range(K)]
    return client_data

# Assuming vit_small and device are defined in a previous cell and accessible

# FlowerClient class
class FlowerClient(fl.client.NumPyClient):
    def __init__(self, cid, trainloader, valloader, masks):
        self.model = vit_small(patch_size=16, num_classes=100).to(device)
        # Assuming SparseSGDM is defined in a previous cell and accessible
        self.optimizer = SparseSGDM(self.model.parameters(), lr=0.01, momentum=0.9, gradient_masks=masks)
        self.trainloader = trainloader
        self.valloader = valloader

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        self.model.train()
        # Assuming scaler is defined in a previous cell and accessible
        for _ in range(4):  # J=4 local steps
            for inputs, labels in self.trainloader:
                inputs, labels = inputs.to(device), labels.to(device)
                self.optimizer.zero_grad()
                with autocast():
                    outputs = self.model(inputs)
                    loss = nn.CrossEntropyLoss()(outputs, labels)
                scaler.scale(loss).backward()
                scaler.step(self.optimizer)
                scaler.update()
        return self.get_parameters(config={}), len(self.trainloader.dataset), {}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        self.model.eval()
        loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for inputs, labels in self.valloader:
                inputs, labels = inputs.to(device), labels.to(device)
                with autocast():
                    outputs = self.model(inputs)
                    loss += nn.CrossEntropyLoss()(outputs, labels).item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        return float(loss / len(self.valloader)), len(self.valloader.dataset), {"accuracy": float(correct / total)}

# Create IID clients and loaders
clients_iid = iid_sharding(trainset, K=100)
client_loaders = [(DataLoader(clients_iid[i], batch_size=128, shuffle=True, num_workers=2, pin_memory=True),
                   DataLoader(valset, batch_size=128, num_workers=2, pin_memory=True)) for i in range(100)]

# Client function
def client_fn(cid: str):
    # Assuming masks is defined in a previous cell and accessible
    return FlowerClient(int(cid), *client_loaders[int(cid)], masks)  # Pass your calibrated masks

# Start simulation (50 rounds; reduce to 10-20 if testing)
fl.simulation.start_simulation(
    client_fn=client_fn,
    num_clients=100,
    client_resources={"num_cpus": 1, "num_gpus": 1.0},
    config=fl.server.ServerConfig(num_rounds=10)
)

# Save final global model (add after simulation if needed)
# Assuming model is accessible here (it's defined within the client, so need to adapt)
# For saving the final model, you would typically get the aggregated parameters from the server
# and load them into a model instance after the simulation.
# Example (would need to retrieve parameters from simulation result):
# final_parameters = simulation_result.parameters # This is conceptual; need to check Flower docs
# final_model = vit_small(patch_size=16, num_classes=100)
# params_dict = zip(final_model.state_dict().keys(), final_parameters)
# state_dict = {k: torch.tensor(v) for k, v in params_dict}
# final_model.load_state_dict(state_dict, strict=True)
# torch.save(final_model.state_dict(), '/content/drive/MyDrive/fl_project/fedavg_iid_model.pth')

print("Simulation completed.")

"""## Identify remaining steps

### Subtask:
Retry running the federated learning simulation after addressing potential environment issues causing the Ray import error.

**Reasoning**:
Execute the cell with the pip install command to ensure Ray and other simulation dependencies are installed.
"""

!pip install -U "flwr[simulation]"

"""**Reasoning**:
The dependencies are confirmed to be installed. Now, execute the cell containing the data loading, sharding, client definition, and simulation start code to attempt the simulation again.


"""

from torch.utils.data import DataLoader
import flwr as fl
import torch.nn as nn
from torch.cuda.amp import autocast
from torch.utils.data import random_split, Subset
import numpy as np

# Transforms for CIFAR-100 (assuming they are needed)
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
])

# Download datasets (assuming they are needed and not already downloaded)
import torchvision.datasets as datasets
full_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)

# Split train into train (45k) and val (5k)
trainset, valset = random_split(full_trainset, [45000, 5000])

# Function for IID sharding (K clients)
def iid_sharding(dataset, K):
    num_samples = len(dataset)
    samples_per_client = num_samples // K
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    client_data = [Subset(dataset, indices[i*samples_per_client:(i+1)*samples_per_client]) for i in range(K)]
    return client_data

# Assuming vit_small and device are defined in a previous cell and accessible

# FlowerClient class
class FlowerClient(fl.client.NumPyClient):
    def __init__(self, cid, trainloader, valloader, masks):
        self.model = vit_small(patch_size=16, num_classes=100).to(device)
        # Assuming SparseSGDM is defined in a previous cell and accessible
        self.optimizer = SparseSGDM(self.model.parameters(), lr=0.01, momentum=0.9, gradient_masks=masks)
        self.trainloader = trainloader
        self.valloader = valloader

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        self.model.train()
        # Assuming scaler is defined in a previous cell and accessible
        for _ in range(4):  # J=4 local steps
            for inputs, labels in self.trainloader:
                inputs, labels = inputs.to(device), labels.to(device)
                self.optimizer.zero_grad()
                with autocast():
                    outputs = self.model(inputs)
                    loss = nn.CrossEntropyLoss()(outputs, labels)
                scaler.scale(loss).backward()
                scaler.step(self.optimizer)
                scaler.update()
        return self.get_parameters(config={}), len(self.trainloader.dataset), {}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        self.model.eval()
        loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for inputs, labels in self.valloader:
                inputs, labels = inputs.to(device), labels.to(device)
                with autocast():
                    outputs = self.model(inputs)
                    loss += nn.CrossEntropyLoss()(outputs, labels).item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        return float(loss / len(self.valloader)), len(self.valloader.dataset), {"accuracy": float(correct / total)}

# Create IID clients and loaders
clients_iid = iid_sharding(trainset, K=100)
client_loaders = [(DataLoader(clients_iid[i], batch_size=128, shuffle=True, num_workers=2, pin_memory=True),
                   DataLoader(valset, batch_size=128, num_workers=2, pin_memory=True)) for i in range(100)]

# Client function
def client_fn(cid: str):
    # Assuming masks is defined in a previous cell and accessible
    return FlowerClient(int(cid), *client_loaders[int(cid)], masks)  # Pass your calibrated masks

# Start simulation (50 rounds; reduce to 10-20 if testing)
fl.simulation.start_simulation(
    client_fn=client_fn,
    num_clients=100,
    client_resources={"num_cpus": 1, "num_gpus": 1.0},
    config=fl.server.ServerConfig(num_rounds=10) # Reduced to 10 rounds for quicker test
)

# Save final global model (add after simulation if needed)
# Assuming model is accessible here (it's defined within the client, so need to adapt)
# For saving the final model, you would typically get the aggregated parameters from the server
# and load them into a model instance after the simulation.
# Example (would need to retrieve parameters from simulation result):
# final_parameters = simulation_result.parameters # This is conceptual; need to check Flower docs
# final_model = vit_small(patch_size=16, num_classes=100)
# params_dict = zip(final_model.state_dict().keys(), final_parameters)
# state_dict = {k: torch.tensor(v) for k, v in params_dict}
# final_model.load_state_dict(state_dict, strict=True)
# torch.save(final_model.state_dict(), '/content/drive/MyDrive/fl_project/fedavg_iid_model.pth')

print("Simulation completed.")

"""## Identify remaining steps

### Subtask:
Retry running the federated learning simulation for the third time.

**Reasoning**:
Attempt to run the federated learning simulation for the third time after the previous failed attempts to resolve the Ray import error.
"""

from torch.utils.data import DataLoader
import flwr as fl
import torch.nn as nn
from torch.cuda.amp import autocast
from torch.utils.data import random_split, Subset
import numpy as np

# Transforms for CIFAR-100 (assuming they are needed)
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
])

# Download datasets (assuming they are needed and not already downloaded)
import torchvision.datasets as datasets
full_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)

# Split train into train (45k) and val (5k)
trainset, valset = random_split(full_trainset, [45000, 5000])

# Function for IID sharding (K clients)
def iid_sharding(dataset, K):
    num_samples = len(dataset)
    samples_per_client = num_samples // K
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    client_data = [Subset(dataset, indices[i*samples_per_client:(i+1)*samples_per_client]) for i in range(K)]
    return client_data

# Assuming vit_small and device are defined in a previous cell and accessible

# FlowerClient class
class FlowerClient(fl.client.NumPyClient):
    def __init__(self, cid, trainloader, valloader, masks):
        self.model = vit_small(patch_size=16, num_classes=100).to(device)
        # Assuming SparseSGDM is defined in a previous cell and accessible
        self.optimizer = SparseSGDM(self.model.parameters(), lr=0.01, momentum=0.9, gradient_masks=masks)
        self.trainloader = trainloader
        self.valloader = valloader

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        self.model.train()
        # Assuming scaler is defined in a previous cell and accessible
        for _ in range(4):  # J=4 local steps
            for inputs, labels in self.trainloader:
                inputs, labels = inputs.to(device), labels.to(device)
                self.optimizer.zero_grad()
                with autocast():
                    outputs = self.model(inputs)
                    loss = nn.CrossEntropyLoss()(outputs, labels)
                scaler.scale(loss).backward()
                scaler.step(self.optimizer)
                scaler.update()
        return self.get_parameters(config={}), len(self.trainloader.dataset), {}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        self.model.eval()
        loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for inputs, labels in self.valloader:
                inputs, labels = inputs.to(device), labels.to(device)
                with autocast():
                    outputs = self.model(inputs)
                    loss += nn.CrossEntropyLoss()(outputs, labels).item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        return float(loss / len(self.valloader)), len(self.valloader.dataset), {"accuracy": float(correct / total)}

# Create IID clients and loaders
clients_iid = iid_sharding(trainset, K=100)
client_loaders = [(DataLoader(clients_iid[i], batch_size=128, shuffle=True, num_workers=2, pin_memory=True),
                   DataLoader(valset, batch_size=128, num_workers=2, pin_memory=True)) for i in range(100)]

# Client function
def client_fn(cid: str):
    # Assuming masks is defined in a previous cell and accessible
    return FlowerClient(int(cid), *client_loaders[int(cid)], masks)  # Pass your calibrated masks

# Start simulation (Reduced to 10 rounds for quicker test)
print("Starting Flower simulation for the third attempt...")
fl.simulation.start_simulation(
    client_fn=client_fn,
    num_clients=100,
    client_resources={"num_cpus": 1, "num_gpus": 1.0},
    config=fl.server.ServerConfig(num_rounds=10)
)

print("Simulation completed.")

"""## Summary:

### Data Analysis Key Findings

*   The initial steps of environment setup, data loading and processing (CIFAR-100, train/val/test split, sharding functions), model readiness, sparse optimizer definition and mask calibration, and Federated Learning Client class definition appear to be completed based on the executed notebook cells.
*   Multiple attempts to run the Federated Learning simulation using the Flower library resulted in a persistent `ImportError: Unable to import module ray`, preventing the simulation from starting.
*   Despite verifying that `flwr[simulation]` (which includes `ray`) was installed, the import error persisted across several retries, suggesting an environment-specific issue rather than a simple missing package.

### Insights or Next Steps

*   The critical next step is to resolve the `ImportError` for `ray`. This may require restarting the notebook runtime and re-running all cells or investigating potential conflicts or configuration issues within the execution environment.
*   Once the simulation can run, the next steps will involve completing the specified number of rounds, evaluating the performance of the federated model, and proceeding to the model editing phase as outlined in the project requirements document.

## Identify remaining steps

### Subtask:
Retry running the federated learning simulation after addressing potential environment issues causing the Ray import error.
"""

from torch.utils.data import DataLoader
import flwr as fl
import torch.nn as nn
from torch.cuda.amp import autocast
from torch.utils.data import random_split, Subset
import numpy as np

# Transforms for CIFAR-100 (assuming they are needed)
import torchvision.transforms as transforms
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))
])

# Download datasets (assuming they are needed and not already downloaded)
import torchvision.datasets as datasets
full_trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)
testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)

# Split train into train (45k) and val (5k)
trainset, valset = random_split(full_trainset, [45000, 5000])

# Function for IID sharding (K clients)
def iid_sharding(dataset, K):
    num_samples = len(dataset)
    samples_per_client = num_samples // K
    indices = np.arange(num_samples)
    np.random.shuffle(indices)
    client_data = [Subset(dataset, indices[i*samples_per_client:(i+1)*samples_per_client]) for i in range(K)]
    return client_data

# Assuming vit_small and device are defined in a previous cell and accessible

# FlowerClient class
class FlowerClient(fl.client.NumPyClient):
    def __init__(self, cid, trainloader, valloader, masks):
        self.model = vit_small(patch_size=16, num_classes=100).to(device)
        # Assuming SparseSGDM is defined in a previous cell and accessible
        self.optimizer = SparseSGDM(self.model.parameters(), lr=0.01, momentum=0.9, gradient_masks=masks)
        self.trainloader = trainloader
        self.valloader = valloader

    def get_parameters(self, config):
        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]

    def set_parameters(self, parameters):
        params_dict = zip(self.model.state_dict().keys(), parameters)
        state_dict = {k: torch.tensor(v) for k, v in params_dict}
        self.model.load_state_dict(state_dict, strict=True)

    def fit(self, parameters, config):
        self.set_parameters(parameters)
        self.model.train()
        # Assuming scaler is defined in a previous cell and accessible
        for _ in range(4):  # J=4 local steps
            for inputs, labels in self.trainloader:
                inputs, labels = inputs.to(device), labels.to(device)
                self.optimizer.zero_grad()
                with autocast():
                    outputs = self.model(inputs)
                    loss = nn.CrossEntropyLoss()(outputs, labels)
                scaler.scale(loss).backward()
                scaler.step(self.optimizer)
                scaler.update()
        return self.get_parameters(config={}), len(self.trainloader.dataset), {}

    def evaluate(self, parameters, config):
        self.set_parameters(parameters)
        self.model.eval()
        loss, correct, total = 0, 0, 0
        with torch.no_grad():
            for inputs, labels in self.valloader:
                inputs, labels = inputs.to(device), labels.to(device)
                with autocast():
                    outputs = self.model(inputs)
                    loss += nn.CrossEntropyLoss()(outputs, labels).item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        return float(loss / len(self.valloader)), len(self.valloader.dataset), {"accuracy": float(correct / total)}

# Create IID clients and loaders
clients_iid = iid_sharding(trainset, K=100)
client_loaders = [(DataLoader(clients_iid[i], batch_size=128, shuffle=True, num_workers=2, pin_memory=True),
                   DataLoader(valset, batch_size=128, num_workers=2, pin_memory=True)) for i in range(100)]

# Client function
def client_fn(cid: str):
    # Assuming masks is defined in a previous cell and accessible
    return FlowerClient(int(cid), *client_loaders[int(cid)], masks)  # Pass your calibrated masks

# Start simulation (50 rounds; reduce to 10-20 if testing)
fl.simulation.start_simulation(
    client_fn=client_fn,
    num_clients=100,
    client_resources={"num_cpus": 1, "num_gpus": 1.0},
    config=fl.server.ServerConfig(num_rounds=10) # Reduced to 10 rounds for quicker test
)

# Save final global model (add after simulation if needed)
# Assuming model is accessible here (it's defined within the client, so need to adapt)
# For saving the final model, you would typically get the aggregated parameters from the server
# and load them into a model instance after the simulation.
# Example (would need to retrieve parameters from simulation result):
# final_parameters = simulation_result.parameters # This is conceptual; need to check Flower docs
# final_model = vit_small(patch_size=16, num_classes=100)
# params_dict = zip(final_model.state_dict().keys(), final_parameters)
# state_dict = {k: torch.tensor(v) for k, v in params_dict}
# final_model.load_state_dict(state_dict, strict=True)
# torch.save(final_model.state_dict(), '/content/drive/MyDrive/fl_project/fedavg_iid_model.pth')

print("Simulation completed.")